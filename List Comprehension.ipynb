{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cebbbc1-8d03-452e-aab1-bf299b3daf80",
   "metadata": {},
   "source": [
    "#### Understanding List Comprehension\n",
    "- [word for word in tokens if word.lower() not in stop_words]\n",
    "- This is called List Comprehension in Python, which is a concise way to create a new list by \n",
    "iterating over an existing iterable and applying a condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c813c81-d002-48c7-9b6b-eb70d9080053",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "new_list = [expression for item in iterable of condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94950d99-0be9-4c23-8e41-8aa2070dfc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = []\n",
    "for fruit in basket:\n",
    " if len(fruit) > 5:\n",
    "    long_fruits.append(fruit)\n",
    " \n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d887891-b5e7-4681-ab62-be7b58602979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# With List Comprehension\n",
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = [fruit for fruit in basket if len(fruit) > 5]\n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a2f590-1c54-466f-b09e-bfd470f6db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Other example\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "doubled = [num * 2 for num in numbers]\n",
    "print(doubled)\n",
    "[2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe2863-09fe-4300-871d-6c00e56d46be",
   "metadata": {},
   "source": [
    "#### Removing Punctuation\n",
    "Punctuation can often be noise in text processing tasks, so we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359bc82-e2a8-468a-8bc7-242841028927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuation\n",
    "reviews_no_punct = [[word for word in tokens if word not in string.punctuation] for tokens in filtered_tokens]\n",
    "print(reviews_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a931e7f-9bf0-4f5d-8e90-585d72f4c298",
   "metadata": {},
   "source": [
    "#### Removing Special Characters: \n",
    "Special characters such as “#”, “@”, or emoji may need to be removed or treated based on their relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd43e62-8aaa-4cd0-9547-4edc9224463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_chars(tokens): return [re.sub(r'[^A-Za-z0-9]+', '', word) for word in tokens]\n",
    "reviews_cleaned = [remove_special_chars(tokens) for tokens in filtered_tokens]\n",
    "print(reviews_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b61f9-2d21-4650-bd7b-3f1d4e3e51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # Stemming algorithm\n",
    "from nltk.tokenize import word_tokenize # For breaking text into tokens\n",
    "\n",
    "# The Porter Stemmer is a popular algorithm for stemming in English.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample Text\n",
    "text = \"I am loving the process of learning and understanding NLP concepts.\"\n",
    "\n",
    "# Tokenize the Text\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# apply stemming to each word in the list of tokens\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb8225-2f3e-463f-9ced-af29c56320d4",
   "metadata": {},
   "source": [
    "#### Wordnet:\n",
    "- wordnet refers to the WordNet lexical database, a large database of English words developed by Princeton University. It groups words into sets of synonyms called synsets and provides semantic relationships between these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f354b-5b30-4547-bd1e-d790dfd7517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required Libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# We need WordNet and Punkt for tokenization and lemmatization\n",
    "nltk.download('wordnet') # For lexical database\n",
    "nltk.download('omw-1.4') # Optional for extended multilingual support\n",
    "nltk.download('punkt') # For tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4b2b5-bdcc-4bc6-ab58-f4817342dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235d827-741a-4605-b37a-8d80495c1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The leaves on the tree were falling. She was running quickly but got tired.\"\n",
    "\n",
    "# Break sentence into words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75cfc9-10e9-4abf-97aa-58fc8102fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1211c3e-3dff-40ea-9af5-9272df3625de",
   "metadata": {},
   "source": [
    "#### Lemmatization with POS Tagging:\n",
    "- When paired with Part-of-Speech (POS) tagging, the lemmatizer gains context about how the word is used (noun, verb, adjective, etc.), resulting in more accurate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab39a3-1d90-4654-86e7-aa9255271d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization (with POS tagging)\n",
    "\n",
    "# Import necessary libraries\n",
    "from nltk.corpus import wordnet # for WordNet-compatible POS tags\n",
    "from nltk.tag import pos_tag # to assign POS tags to words\n",
    "from nltk.stem import WordNetLemmatizer # from NLTK for lemmatization\n",
    "\n",
    "# Function to map POS tags to WordNet tags\n",
    "def get_wordnet_pos(word): tag = pos_tag([word])[0][1][0].upper() # Get the POS tag's first letter\n",
    "tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "\n",
    "# dict ={ \"key\" : values, \"key\":value....}\n",
    "return tag_dict.get(tag, wordnet.NOUN) # Default to noun if tag is not in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2aaba-62b4-4e84-9493-b6e7f157445b",
   "metadata": {},
   "source": [
    "#### Lemmatization with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811c686-02a8-4cd3-8527-84d1ce1f6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The leaves are falling quickly from the trees, and the children are happily playing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform lemmatization with POS tagging\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "# Output the result\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ec276-126c-4b77-a30d-26d2da4ed8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find synonyms of a word\n",
    "synonyms = wordnet.synsets(\"run\")\n",
    "print(synonyms[0].definition())\n",
    "\n",
    "# find synonyms and displays the definition of the first synonym for the word \"run\"\n",
    "a score in baseball made by a runner touching all four bases safely\n",
    "\n",
    "# Import libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, download\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1fec4c-facd-431d-86a9-68d41926b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The leaves are falling quickly from the trees, and the children are happily playing.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered:\", filtered_tokens)\n",
    "\n",
    "# Step 3: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed:\", stemmed_tokens)\n",
    "\n",
    "# Step 4: Lemmatization (without POS tagging)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized (without POS):\", lemmatized_tokens)\n",
    "\n",
    "# Step 5: Lemmatization (with POS tagging)\n",
    "def get_wordnet_pos(word):\n",
    "tag = pos_tag([word])[0][1][0].upper() # Get the POS tag\n",
    "tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV} return tag_dict.get(tag, wordnet.NOUN) # Default to noun\n",
    "lemmatized_tokens_with_pos = [lemmatizer.lemmatize(word, \n",
    "get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "print(\"Lemmatized (with POS):\", lemmatized_tokens_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56455860-00d0-462a-b26f-1a5a2b18f282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
