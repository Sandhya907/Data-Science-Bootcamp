{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7005a53-1d62-47f7-b42d-2d7a37d7c44d",
   "metadata": {},
   "source": [
    "#### Text Preprocessing in Python\n",
    "- Python Libraries like NLTK and spaCy are powerful tools for text preprocessing in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b7532-ea5a-41e2-b948-517312a37d1a",
   "metadata": {},
   "source": [
    "### 1. Tokenization \n",
    "- Word Tokenization\n",
    "- Sentence Tokenization\n",
    "- Character Tokenization\n",
    "- Whitespace Tokenization\n",
    "- Punctuation-Aware Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa53c13-daab-41d6-8907-acf72b74cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8048e9f5-c957-4fdd-a4e1-6bd91272958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sandh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') ## This downloads the necessary data for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba5b952-4acd-407d-8413-42e921c22fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love programming!\"\n",
    "\n",
    "text\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7389ac4-3afa-4277-8cb4-8935bab765e9",
   "metadata": {},
   "source": [
    "- NLTK (Natural Language ToolKit) is a popular library in Python used for Natural Language Processing (NLP).\n",
    "- punkt_tab is a pre-trained tokenizer model is available in NLTK, which is used for braking down text into words or sentences. It is a necessary resource for tokenization tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27024ad-a019-4f9e-9db4-faaff9d94291",
   "metadata": {},
   "source": [
    "#### Sentence tokenization splits a text into sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98803ec-1bbc-4cd5-aa85-0a8bfbb42bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"I love programming. It's fun!\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)\n",
    "\n",
    "Sentence Tokens: ['I love programming.', \"It's fun!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c90e3-a1e8-4c32-a639-a2b6a0956039",
   "metadata": {},
   "source": [
    "##### Character tokenization splits text into individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e398d-41bf-4544-ada3-69845f7abd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example word\n",
    "word = \"programming\"\n",
    "\n",
    "# Character Tokenization (manual splitting)\n",
    "char_tokens = list(word)\n",
    "\n",
    "# Print the result\n",
    "print(\"Character Tokenization:\", char_tokens)\n",
    "\n",
    "Character Tokenization: ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f4a7b-f6bb-4e9e-9e4e-d2e4a039b375",
   "metadata": {},
   "source": [
    "#### Whitespace tokenization splits the text based on spaces (whitespace characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06a47a-954a-4c33-91da-091acdcc07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "text = \"I love programming.\"\n",
    "\n",
    "# Whitespace Tokenization (splitting based on space)\n",
    "whitespace_tokens = text.split( )\n",
    "\n",
    "# Print the result\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)\n",
    "\n",
    "Whitespace Tokenization: ['I', 'love', 'programming.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf91732-2b47-450c-939f-41e534e375ec",
   "metadata": {},
   "source": [
    "#### Punctuation-Aware Tokenization handles punctuation separately from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407fe3f-afa9-4d2e-b734-e0b6a2c3f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence with punctuation\n",
    "text = \"Hello! How are you doing?\"\n",
    "\n",
    "# Word Tokenization (with punctuation handling)\n",
    "word_tokens_with_punct = word_tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Punctuation-Aware Tokenization:\", word_tokens_with_punct)\n",
    "Punctuation-Aware Tokenization: ['Hello', '!', 'How', 'are', 'you', 'doing', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd64837-efb4-43c9-8f32-80ab8e4ba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3331805-0057-4966-93ad-748c4086abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'programming', 'and', 'it', 'is', 'fun']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentence (tokens)\n",
    "tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e882f290-dfc0-4aac-9eff-f93153864fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca5cc2f-2338-4fbd-bee0-610bcc0c4eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90733389-f4fd-4f93-b10e-a73018d2f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words List: {'being', 'to', 'until', 'their', 'when', 'ourselves', 'because', 'm', 'which', 'not', 'this', 'didn', 'down', 'there', 'few', 'o', 'at', 'who', 'any', 'ours', 'yourselves', 'other', \"it's\", 'theirs', \"isn't\", 'was', 'more', \"needn't\", \"hadn't\", 'has', 'she', \"should've\", 'with', 'll', 'weren', 'after', 'did', 'd', 'those', 'yourself', 'it', 'a', 'while', 'isn', 'for', 'nor', 'as', 'during', 'what', 'own', 'he', \"shouldn't\", 'between', 'is', 'very', \"you'd\", 's', 'whom', \"didn't\", \"shan't\", \"aren't\", 'again', 'over', \"mightn't\", \"won't\", 'further', 'most', 'had', 'from', \"haven't\", 'won', 'were', 'ain', 'her', 'herself', 'so', 'such', \"she's\", \"mustn't\", \"you're\", 'into', 'just', 'me', 'doing', 'an', 'wasn', 'do', 'but', \"don't\", 'am', 'through', 'its', 'where', 'under', 're', 'why', 'below', 'or', \"you've\", 'both', 'the', 'having', 'yours', 'our', 'up', 'aren', 'about', 'wouldn', 'him', 'your', 'are', \"wouldn't\", \"hasn't\", 'himself', 'some', \"weren't\", 'no', 'should', 'now', 'only', 'we', 'before', 'then', 'hasn', 'shouldn', 'in', 'mustn', 'on', 'same', 'all', 'you', \"you'll\", 'these', 'be', 'by', 'too', 'have', 'out', 'does', 'haven', 'don', 'ma', 'of', 'themselves', 'that', \"that'll\", 'and', 'each', 'my', 'needn', \"couldn't\", 'than', \"doesn't\", 'hadn', 'once', \"wasn't\", 'couldn', 'here', 'itself', 'them', 'they', 'how', 'shan', 'myself', 'y', 'been', 'will', 'his', 'can', 'above', 'mightn', 'against', 've', 'i', 'off', 'doesn', 'if', 't', 'hers'}\n"
     ]
    }
   ],
   "source": [
    "# Print the stop words list (optional)\n",
    "print(\"Stop Words List:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f915a66d-6214-455d-9e26-59730bf801ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab4b983-d7e1-44f5-9945-90605112a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ce2189e-2620-4b86-b7d1-ec1ba9547c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Word Removal: ['love', 'programming', 'fun']\n"
     ]
    }
   ],
   "source": [
    "# Print the filtered tokens\n",
    "#tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "print(\"Tokens after Stop Word Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b6095-0a14-432b-bfdf-747c46987ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
